To display the current passage in the transcription text in sync with audio playback while using OpenAI’s Whisper, you can implement a simple setup with the following components:



Overview

1. Transcribe the audio file: Use Whisper to generate timestamps for each segment of text.

2. Audio playback: Use an audio player to play the audio file.

3. Sync transcription display: Show the corresponding passage in real time as the audio plays.



Steps to Implement



1. Transcribe the Audio with Timestamps



Whisper’s output includes timestamps for each segment of text. You’ll need to run the transcription and save the output with timestamps.



import whisper



# Load the Whisper model

model = whisper.load_model("base")  # Replace "base" with the desired model size



# Transcribe the audio file with timestamps

result = model.transcribe("audio_file.mp3", word_timestamps=True)



# Save the transcription

with open("transcription.json", "w") as f:

    import json

    json.dump(result, f, indent=4)



print("Transcription complete with timestamps!")



This will give you output like this (simplified):



{

  "segments": [

    {

      "start": 0.0,

      "end": 4.2,

      "text": "This is the first sentence."

    },

    {

      "start": 4.2,

      "end": 8.7,

      "text": "And here is the second sentence."

    }

  ]

}



2. Set Up Audio Playback



You can use a Python library like PyDub or pydub.playback to play the audio file. Another option is pygame for more control over playback and synchronization.



Here’s an example with pygame:



import pygame

import time

import json



# Load the transcription file

with open("transcription.json", "r") as f:

    transcription = json.load(f)



# Initialize pygame mixer

pygame.mixer.init()

pygame.mixer.music.load("audio_file.mp3")



# Start playing the audio

pygame.mixer.music.play()



# Sync text with audio

for segment in transcription['segments']:

    while pygame.mixer.music.get_pos() / 1000 < segment['start']:

        time.sleep(0.1)  # Wait until the audio reaches the current segment

    print(segment['text'])  # Replace this with your code to display text in a UI



3. Enhance with a Graphical Interface



To create a better user experience, you can use a GUI framework like Tkinter, PyQt, or Streamlit to display the transcription dynamically.



Example using Tkinter:



import tkinter as tk

from tkinter import ttk

import pygame

import time

import json



# Load the transcription file

with open("transcription.json", "r") as f:

    transcription = json.load(f)



# Set up the GUI

root = tk.Tk()

root.title("Live Transcription")

transcription_label = ttk.Label(root, text="", wraplength=500, font=("Arial", 14))

transcription_label.pack(pady=20)



# Initialize pygame mixer

pygame.mixer.init()

pygame.mixer.music.load("audio_file.mp3")



def play_audio_with_sync():

    pygame.mixer.music.play()

    for segment in transcription['segments']:

        while pygame.mixer.music.get_pos() / 1000 < segment['start']:

            time.sleep(0.1)  # Wait until the audio reaches the current segment

        transcription_label.config(text=segment['text'])

        root.update()  # Update the GUI to show the new text



# Play audio with synchronized transcription

play_audio_with_sync()



root.mainloop()



4. Key Features to Add

• Scroll through text: Allow users to scroll back and see previous passages.

• Highlight active text: Use visual effects (e.g., bold text or color) to highlight the current passage.

• Pause and Resume: Add buttons for audio playback control.



5. Optional: Use Existing Tools



If you’d prefer not to build this yourself, tools like Streamlit allow you to quickly create a browser-based app for real-time transcription display.



Would you like detailed code for one specific feature (e.g., highlighting text or audio controls)?



