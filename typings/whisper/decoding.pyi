"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional, Sequence, TYPE_CHECKING, Tuple, Union
from torch import Tensor
from .tokenizer import Tokenizer
from .utils import compression_ratio
from .model import Whisper

if TYPE_CHECKING:
    ...
@torch.no_grad()
def detect_language(model: Whisper, mel: Tensor, tokenizer: Tokenizer = ...) -> Tuple[Tensor, List[dict]]:
    """
    Detect the spoken language in the audio, and return them as list of strings, along with the ids
    of the most probable language tokens and the probability distribution over all language tokens.
    This is performed outside the main decode loop in order to not interfere with kv-caching.

    Returns
    -------
    language_tokens : Tensor, shape = (n_audio,)
        ids of the most probable language tokens, which appears after the startoftranscript token.
    language_probs : List[Dict[str, float]], length = n_audio
        list of dictionaries containing the probability distribution over all languages.
    """
    ...

@dataclass(frozen=True)
class DecodingOptions:
    task: str = ...
    language: Optional[str] = ...
    temperature: float = ...
    sample_len: Optional[int] = ...
    best_of: Optional[int] = ...
    beam_size: Optional[int] = ...
    patience: Optional[float] = ...
    length_penalty: Optional[float] = ...
    prompt: Optional[Union[str, List[int]]] = ...
    prefix: Optional[Union[str, List[int]]] = ...
    suppress_tokens: Optional[Union[str, Iterable[int]]] = ...
    suppress_blank: bool = ...
    without_timestamps: bool = ...
    max_initial_timestamp: Optional[float] = ...
    fp16: bool = ...


@dataclass(frozen=True)
class DecodingResult:
    audio_features: Tensor
    language: str
    language_probs: Optional[Dict[str, float]] = ...
    tokens: List[int] = ...
    text: str = ...
    avg_logprob: float = ...
    no_speech_prob: float = ...
    temperature: float = ...
    compression_ratio: float = ...


class Inference:
    def logits(self, tokens: Tensor, audio_features: Tensor) -> Tensor:
        """Perform a forward pass on the decoder and return per-token logits"""
        ...
    
    def rearrange_kv_cache(self, source_indices) -> None:
        """Update the key-value cache according to the updated beams"""
        ...
    
    def cleanup_caching(self) -> None:
        """Clean up any resources or hooks after decoding is finished"""
        ...
    


class PyTorchInference(Inference):
    def __init__(self, model: Whisper, initial_token_length: int) -> None:
        ...
    
    def logits(self, tokens: Tensor, audio_features: Tensor) -> Tensor:
        ...
    
    def cleanup_caching(self): # -> None:
        ...
    
    def rearrange_kv_cache(self, source_indices): # -> None:
        ...
    


class SequenceRanker:
    def rank(self, tokens: List[List[Tensor]], sum_logprobs: List[List[float]]) -> List[int]:
        """
        Given a list of groups of samples and their cumulative log probabilities,
        return the indices of the samples in each group to select as the final result
        """
        ...
    


class MaximumLikelihoodRanker(SequenceRanker):
    """
    Select the sample with the highest log probabilities, penalized using either
    a simple length normalization or Google NMT paper's length penalty
    """
    def __init__(self, length_penalty: Optional[float]) -> None:
        ...
    
    def rank(self, tokens: List[List[Tensor]], sum_logprobs: List[List[float]]): # -> list[intp]:
        ...
    


class TokenDecoder:
    def reset(self): # -> None:
        """Initialize any stateful variables for decoding a new sequence"""
        ...
    
    def update(self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor) -> Tuple[Tensor, bool]:
        """Specify how to select the next token, based on the current trace and logits

        Parameters
        ----------
        tokens : Tensor, shape = (n_batch, current_sequence_length)
            all tokens in the context so far, including the prefix and sot_sequence tokens

        logits : Tensor, shape = (n_batch, vocab_size)
            per-token logits of the probability distribution at the current step

        sum_logprobs : Tensor, shape = (n_batch)
            cumulative log probabilities for each sequence

        Returns
        -------
        tokens : Tensor, shape = (n_batch, current_sequence_length + 1)
            the tokens, appended with the selected next token

        completed : bool
            True if all sequences has reached the end of text

        """
        ...
    
    def finalize(self, tokens: Tensor, sum_logprobs: Tensor) -> Tuple[Sequence[Sequence[Tensor]], List[List[float]]]:
        """Finalize search and return the final candidate sequences

        Parameters
        ----------
        tokens : Tensor, shape = (n_audio, n_group, current_sequence_length)
            all tokens in the context so far, including the prefix and sot_sequence

        sum_logprobs : Tensor, shape = (n_audio, n_group)
            cumulative log probabilities for each sequence

        Returns
        -------
        tokens : Sequence[Sequence[Tensor]], length = n_audio
            sequence of Tensors containing candidate token sequences, for each audio input

        sum_logprobs : List[List[float]], length = n_audio
            sequence of cumulative log probabilities corresponding to the above

        """
        ...
    


class GreedyDecoder(TokenDecoder):
    def __init__(self, temperature: float, eot: int) -> None:
        ...
    
    def update(self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor) -> Tuple[Tensor, bool]:
        ...
    
    def finalize(self, tokens: Tensor, sum_logprobs: Tensor): # -> tuple[Tensor, List[Any]]:
        ...
    


class BeamSearchDecoder(TokenDecoder):
    def __init__(self, beam_size: int, eot: int, inference: Inference, patience: Optional[float] = ...) -> None:
        ...
    
    def reset(self): # -> None:
        ...
    
    def update(self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor) -> Tuple[Tensor, bool]:
        ...
    
    def finalize(self, preceding_tokens: Tensor, sum_logprobs: Tensor): # -> tuple[list[List[Tensor]], list[List[float]]]:
        ...
    


class LogitFilter:
    def apply(self, logits: Tensor, tokens: Tensor) -> None:
        """Apply any filtering or masking to logits in-place

        Parameters
        ----------
        logits : Tensor, shape = (n_batch, vocab_size)
            per-token logits of the probability distribution at the current step

        tokens : Tensor, shape = (n_batch, current_sequence_length)
            all tokens in the context so far, including the prefix and sot_sequence tokens

        """
        ...
    


class SuppressBlank(LogitFilter):
    def __init__(self, tokenizer: Tokenizer, sample_begin: int) -> None:
        ...
    
    def apply(self, logits: Tensor, tokens: Tensor): # -> None:
        ...
    


class SuppressTokens(LogitFilter):
    def __init__(self, suppress_tokens: Sequence[int]) -> None:
        ...
    
    def apply(self, logits: Tensor, tokens: Tensor): # -> None:
        ...
    


class ApplyTimestampRules(LogitFilter):
    def __init__(self, tokenizer: Tokenizer, sample_begin: int, max_initial_timestamp_index: Optional[int]) -> None:
        ...
    
    def apply(self, logits: Tensor, tokens: Tensor): # -> None:
        ...
    


class DecodingTask:
    inference: Inference
    sequence_ranker: SequenceRanker
    decoder: TokenDecoder
    logit_filters: List[LogitFilter]
    def __init__(self, model: Whisper, options: DecodingOptions) -> None:
        ...
    
    @torch.no_grad()
    def run(self, mel: Tensor) -> List[DecodingResult]:
        ...
    


@torch.no_grad()
def decode(model: Whisper, mel: Tensor, options: DecodingOptions = ..., **kwargs) -> Union[DecodingResult, List[DecodingResult]]:
    """
    Performs decoding of 30-second audio segment(s), provided as Mel spectrogram(s).

    Parameters
    ----------
    model: Whisper
        the Whisper model instance

    mel: torch.Tensor, shape = (80, 3000) or (*, 80, 3000)
        A tensor containing the Mel spectrogram(s)

    options: DecodingOptions
        A dataclass that contains all necessary options for decoding 30-second segments

    Returns
    -------
    result: Union[DecodingResult, List[DecodingResult]]
        The result(s) of decoding contained in `DecodingResult` dataclass instance(s)
    """
    ...

